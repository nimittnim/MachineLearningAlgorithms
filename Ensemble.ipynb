{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ensemble Learning**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble Learning is a very powerful technique in Machine Learning. We use uncorrelated models for prediction and use the majority as final prediction. But, the models used must be:\n",
    "\n",
    "- Uncorrelated\n",
    "- Accurate\n",
    "\n",
    "So, why does this work ? If models are uncorrelated they behave independently. Let $epsilon_{i}$ be the probabilities that model $i$ predicts incorrectly. Then, the probabilty that majority of models fail is \n",
    "\n",
    "P = $ \\Sigma_{J} \\Pi_{k \\in J}\\epsilon_{k}$ \n",
    "\n",
    "where J is subset of modles such that $|J| > n/2 $. Clearly, $P < \\epsilon_{i}$ thus giving better performance.\n",
    "\n",
    "Similarly, for regression the output is the mean of the predictions of each model. Again, if models are independent they are off from decision boundary in orthogonal directions thus giving out True value on average."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take high variance models and try to reduce variance for getting a more generalized decision boundary.\n",
    "Algorithm : \\\n",
    "We get models while training over different subsets of data. Sample Data with replacement and train the model. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take high bias models and try to reduce bias. \n",
    "Algorithm : \\\n",
    "We incrementally build models based on current model. We assign weights to each of the datapoint. Then we train the model over it. Now, we again redistribute the weights based on performance of current model. For the datapoints predicted incorrectly, we increase their weight and decrease the weight of others. We train the model on this new Dataset.\n",
    "\n",
    "Weight Updation : \n",
    "\n",
    "$err_{m} = \\frac{\\Sigma w_{i}(Incorrects)}{\\Sigma w_{i}}$\\\n",
    "$\\alpha_{m} = log\\frac{1-err_{m}}{err_m}$\\\n",
    "For incorrect predictions : $w_{i} = w_{i}e^{alpha_{m}}$\\\n",
    "For correct predictions : $w_{i} = w_{i}e^{-alpha_{m}}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can emply similar strategy for Regression be chooseong appropriate error function to get $\\alpha_{m}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest is the ensebmle of Decision Trees trained over subsets of data over Features spaces and Samples Space."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.1 (v3.11.1:a7a450f84a, Dec  6 2022, 15:24:06) [Clang 13.0.0 (clang-1300.0.29.30)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
