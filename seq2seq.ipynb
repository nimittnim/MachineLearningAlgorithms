{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SEQ2SEQ**\n",
    "\n",
    "Use LSTM and word embeddings to translate one coding to other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordEmbeddings:\n",
    "\n",
    "    def __init__(self,m,r):\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(m,r),\n",
    "            nn.Linear(r,m),\n",
    "        )\n",
    "        self.loss_function = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(),lr = 0.01)\n",
    "        self.unique_words = None\n",
    "\n",
    "    def one_hot_encode_word(self,word):\n",
    "\n",
    "        if (word in self.unique_words):\n",
    "            m = len(self.unique_words)\n",
    "            endcoded_word_index = self.unique_words.index(word)\n",
    "            encoded_word = np.zeros(m)\n",
    "            encoded_word[endcoded_word_index] = 1\n",
    "            return torch.tensor(encoded_word,dtype = torch.float32)\n",
    "        else :\n",
    "            print(\"Query word not in trained words\")\n",
    "    \n",
    "    def generate_encoded_dataset(self,words):\n",
    "\n",
    "        unique_words = list(set(words))\n",
    "        n = len(words)\n",
    "        m = len(unique_words)\n",
    "        X = np.zeros((n,m))\n",
    "        y = np.zeros((n,m))\n",
    "        for i in range(n-1):\n",
    "            j = unique_words.index(words[i])\n",
    "            j_next_word = unique_words.index(words[i+1])\n",
    "            X[i][j] = 1\n",
    "            y[i][j_next_word] = 1 \n",
    "        return torch.tensor(X,dtype = torch.float32),torch.tensor(y,dtype = torch.float32)\n",
    "        \n",
    "    def train(self,X,y):\n",
    "        \n",
    "        max_epochs = 2000\n",
    "        # print(f'Initial Loss {self.loss_function(self.model(X),y)}')\n",
    "        for e in range(max_epochs):\n",
    "            y_hat = self.model(X)\n",
    "            loss = self.loss_function(y_hat,y)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        # print(f'Final Loss {self.loss_function(self.model(X),y)}')\n",
    "\n",
    "    def get_embedding(self,words):\n",
    "\n",
    "        unique_words = list(set(words))\n",
    "        self.unique_words = unique_words\n",
    "        \n",
    "        X,y = self.generate_encoded_dataset(words)\n",
    "        self.train(X,y)\n",
    "        params = []\n",
    "        for param in self.model.parameters():\n",
    "            params.append(param)\n",
    "        embedding = np.array(params[0].detach()).T\n",
    "        return embedding\n",
    "\n",
    "    def next_word(self,word):\n",
    "\n",
    "        if (word in self.unique_words):\n",
    "            softmax = nn.Softmax(dim = 0)\n",
    "            out = np.array(softmax(self.model(self.one_hot_encode_word(word))).detach())\n",
    "            return self.unique_words[np.where(out == np.max(out))[0][0]]    \n",
    "        else :\n",
    "            print(\"Query word not in trained words\") \n",
    "            \n",
    "    def vec2word(self,vec):\n",
    "        ind = np.argmin(np.array(vec.detach()))\n",
    "        return self.unique_words[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM cell\n",
    "\n",
    "class LSTMcell:\n",
    "\n",
    "    def __init__(self,m,r):\n",
    "\n",
    "        # percentage long term to remember\n",
    "        self.w_input_1 = nn.Parameter(torch.rand(r,m),requires_grad=True)\n",
    "        self.w_short_1 = nn.Parameter(torch.rand(r,r),requires_grad=True)\n",
    "        self.b_1 = nn.Parameter(torch.rand(r,1),requires_grad=True)\n",
    "        # sigma\n",
    "\n",
    "        # percentage potential memory to remember\n",
    "        self.w_input_2 = nn.Parameter(torch.rand(r,m),requires_grad=True)\n",
    "        self.w_short_2 = nn.Parameter(torch.rand(r,r),requires_grad=True)\n",
    "        self.b_2 = nn.Parameter(torch.rand(r,1),requires_grad=True)\n",
    "        # sigma\n",
    "\n",
    "        # Potential Long term memory for current input\n",
    "        self.w_input_3 = nn.Parameter(torch.rand(r,m),requires_grad=True)\n",
    "        self.w_short_3 = nn.Parameter(torch.rand(r,r),requires_grad=True)\n",
    "        self.b_3 = nn.Parameter(torch.rand(r,1),requires_grad=True)\n",
    "        # tanh\n",
    "\n",
    "        # New short\n",
    "        self.w_input_4 = nn.Parameter(torch.rand(r,m),requires_grad=True)\n",
    "        self.w_short_4 = nn.Parameter(torch.rand(r,r),requires_grad=True)\n",
    "        self.b_4 = nn.Parameter(torch.rand(r,1),requires_grad=True)\n",
    "        # sigmoid\n",
    "        self.parameters = torch.tensor([self.w_input_1,self.w_input_2,self.w_input_3,self.w_input_4,\n",
    "                           self.w_short_1,self.w_short_2,self.w_short_3,self.w_short_4,\n",
    "                           self.b_1,self.b_2,self.b_3,self.b_4],requires_grad=True)\n",
    "\n",
    "\n",
    "    def integrate(self, input, short, long):\n",
    "        # percentage long term to remember\n",
    "        \n",
    "        o1 = torch.matmul(self.w_input_1, input)\n",
    "        o2 = torch.matmul(self.w_short_1, short)\n",
    "        o3 = torch.mul(long,torch.sigmoid(o1+o2 + self.b_1))\n",
    "\n",
    "        # percentage potential memory to remember\n",
    "        o4 = torch.matmul(self.w_input_2, input)\n",
    "        o5 = torch.matmul(self.w_short_2, short) \n",
    "        o6 = torch.sigmoid(o4+o5 + self.b_2)\n",
    "\n",
    "        # Potential Long term memory for current input\n",
    "        o7 = torch.matmul(self.w_input_3, input)\n",
    "        o8 = torch.matmul(self.w_short_3, short)\n",
    "        o9 = torch.tanh(o7+o8+ self.b_3) \n",
    "\n",
    "        newLong = o3 + torch.mul(o6,o9)\n",
    "        \n",
    "\n",
    "        # new short\n",
    "        o10 = torch.matmul(self.w_input_4, input)\n",
    "        o11 = torch.matmul(self.w_short_4, short)\n",
    "        o12 = torch.sigmoid(o11+o10 + self.b_4)\n",
    "        \n",
    "        newShort = torch.mul(o12,(torch.tanh(newLong)))\n",
    "\n",
    "        return newShort,newLong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.ms = []\n",
    "        self.rs = []\n",
    "        self.parameters = []\n",
    "        self.loss_function = nn.MSELoss()\n",
    "    \n",
    "    def add_layer(self,m,r,n): \n",
    "        # m : Input dimension \n",
    "        # n :number of cells in the layer \n",
    "        # r : output dimension\n",
    "        self.layers.append([])  \n",
    "        self.ms.append(m)\n",
    "        self.rs.append(r)   \n",
    "        for i in range(n):\n",
    "            newcell = LSTMcell(m,r)\n",
    "            self.layers[-1].append(newcell)\n",
    "            self.parameters.append(newcell.parameters)\n",
    "\n",
    "    def forward(self, input):\n",
    "        l = len(self.layers)\n",
    "        short = torch.zeros((l, self.rs[0],1), dtype = torch.float32)\n",
    "        long = torch.zeros((l,self.rs[0],1),dtype = torch.float32)\n",
    "        for i in range(len(input)):\n",
    "            current_input = input[i].unsqueeze(1)\n",
    "            for j in range(l):\n",
    "                short_,long_ = short[j],long[j]\n",
    "                for lstm_cell in self.layers[j]:\n",
    "                    short_,long_ = lstm_cell.integrate(current_input,short_,long_)\n",
    "                short[j],long[j] = short_,long_\n",
    "                current_input = short_\n",
    "        return short_\n",
    "    \n",
    "    def out(self,input):\n",
    "        out = []\n",
    "        for i in range(len(input)):\n",
    "            out.append(self.forward(input[i]))\n",
    "        return torch.tensor(out,requires_grad=True).unsqueeze(-1)\n",
    "    \n",
    "    def train(self,X,y,max_epochs):\n",
    "        optimizer = optim.Adam(self.parameters)\n",
    "        for i in range(max_epochs):\n",
    "            out = self.out(X)\n",
    "            loss = self.loss_function(out,y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq:\n",
    "\n",
    "    def __init__(self,m):\n",
    "        r = 20\n",
    "        self.we = WordEmbeddings(m,r)\n",
    "        self.encoder = LSTM()\n",
    "        self.decoder = LSTM()\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(r,r),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "        self.loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    def encode(self,input):\n",
    "        return self.encoder.forward(input,0,0)\n",
    "    \n",
    "    def feed(self,input):\n",
    "        return self.feed_forward(input)\n",
    "    \n",
    "    def decode(self, short,long):\n",
    "        EOS = self.we.one_hot_encode_word(\"EOS\")\n",
    "        input = EOS.copy()\n",
    "        translation = []\n",
    "        while True:\n",
    "            out = self.feed(self.decoder.forward(input, short, long))\n",
    "            translation.append(out)\n",
    "            if (self.we.unique_words(max(out)) == \"EOS\"):\n",
    "                break\n",
    "        return torch.tensor(translation,dtype = torch.flaot32)\n",
    "    \n",
    "    def generate_embeddings(self,X,y):\n",
    "        X_embeddings = self.we.get_embeddings(X)\n",
    "        y_embeddings = self.we.get_embeddings(y)\n",
    "        return torch.tensor(X_embeddings,dtype=torch.float32),torch.tensor(y_embeddings,dtype=torch.float32)\n",
    "    \n",
    "    def forward(self, X,y):\n",
    "        inp = self.generate_embeddings(X,y)\n",
    "        context_vec =  self.encode(inp)\n",
    "        translation = self.decode(context_vec[0],context_vec[1])\n",
    "        return translation\n",
    "    \n",
    "    def train(self,X,y):\n",
    "        optimizer = nn.optim(self.paramters())\n",
    "        loss_function = self.loss_function()\n",
    "        y_hat = self.forward(X,y)\n",
    "        return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.1 (v3.11.1:a7a450f84a, Dec  6 2022, 15:24:06) [Clang 13.0.0 (clang-1300.0.29.30)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
