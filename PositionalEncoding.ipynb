{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Positional Encoding**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Sequential data, we need to have some kind of context thing that tells how the data is related sequentially. The LSTMs used the previous cell state to influence the current input thus building context. There is one more technique introduced in Attention is all you need - Positional Encoding that does the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000,  1.0000,  0.0000,  1.0000],\n",
      "        [ 0.8415,  0.5403,  0.0998,  0.9950],\n",
      "        [ 0.9093, -0.4161,  0.1987,  0.9801],\n",
      "        [ 0.1411, -0.9900,  0.2955,  0.9553]])\n"
     ]
    }
   ],
   "source": [
    "def getPositionEncoding(seq_len, d, n=10000):  # 10000 in attention paper\n",
    "    P = torch.empty((seq_len, d))\n",
    "    for k in range(seq_len):\n",
    "        for i in range(d//2):\n",
    "            denominator = np.power(n, 2*i/d)\n",
    "            P[k, 2*i] = np.sin(k/denominator)\n",
    "            P[k, 2*i+1] = np.cos(k/denominator)\n",
    "    return P\n",
    " \n",
    "P = getPositionEncoding(seq_len=4, d=4, n=100)\n",
    "print(P)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
